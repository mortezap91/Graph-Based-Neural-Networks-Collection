{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Train CycleGan"
      ],
      "metadata": {
        "id": "p2OhESSIBC8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Library"
      ],
      "metadata": {
        "id": "pbl5HGB8F_Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import itertools\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "import argparse"
      ],
      "metadata": {
        "id": "iOnqFacbGDrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCW7ADkOMhE6",
        "outputId": "73fd99b5-ca21-4057-85e8-b2ece5c96d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def parse_args():\n",
        "desc = \"Pytorch implementation of CinCGAN\"\n",
        "parser = argparse.ArgumentParser(description=desc)\n",
        "parser.add_argument('--gpu', type=str, default='0', help='GPU idx') #0\n",
        "parser.add_argument('--inner_lr', type=float, default=2*1e-4, help='initial inner lr')\n",
        "parser.add_argument('--outer_lr', type=float, default=1e-4, help='initial outer lr')\n",
        "parser.add_argument('--decay_step', type=int, default=40000, help='decay step')\n",
        "parser.add_argument('--bsize', type=int, default=16, help='Batch size')\n",
        "parser.add_argument('--test_bsize', type=int, default=1, help='Test Batch size')\n",
        "parser.add_argument('--iteration', type=int, default=20, help='iteration') #400000\n",
        "parser.add_argument('--eps', type=float, default=1e-8, help='Adam eps')\n",
        "parser.add_argument('--w1', type=float, default=10.0, help='w1')\n",
        "parser.add_argument('--w2', type=float, default=5.0, help='w2')\n",
        "parser.add_argument('--w3', type=float, default=0.5, help='w3')\n",
        "parser.add_argument('--gamma0', type=float, default=1.0, help='gamma0')\n",
        "parser.add_argument('--gamma1', type=float, default=10.0, help='gamma1')\n",
        "parser.add_argument('--gamma2', type=float, default=5.0, help='gamma2')\n",
        "parser.add_argument('--gamma3', type=float, default=2.0, help='gamma3')\n",
        "parser.add_argument('--gamma4', type=float, default=0.0, help='gamma4')\n",
        "parser.add_argument('--train_s_path', type=str, default='/content/drive/MyDrive/Data/Train', help='train source dataset path')\n",
        "parser.add_argument('--train_t_path', type=str, default='/content/drive/MyDrive/Data/Validation', help='train target dataset path')\n",
        "parser.add_argument('--test_s_path', type=str, default='/content/drive/MyDrive/Test /Test_source', help='test source dataset path')\n",
        "parser.add_argument('--test_t_path', type=str, default='/content/drive/MyDrive/Test /Test_target', help='test target dataset path')\n",
        "parser.add_argument('--ngpus_per_node', type=int, default=1, help='ngpus per node')\n",
        "parser.add_argument('--num_workers', type=int, default=0, help='number of workers') #4\n",
        "parser.add_argument('--resume', type=bool, default=False, help='resume or not')\n",
        "parser.add_argument('--resume_iter', type=int, default=1, help='resume iteration')\n",
        "parser.add_argument('--neptune', type=bool, default=False, help='to use neptune or not')\n",
        "parser.add_argument('--save_freq', type=int, default=100, help='checkpoint save frequency') #1000\n",
        "parser.add_argument('--phase', type=str, default='train_inner', help='train_inner / train_outer / test_inner / test_outer')\n",
        "parser.add_argument('--scale_factor', type=int, default=3, help='scale factor : 2 / 4') #4\n",
        "parser.add_argument('--inner_ckpt_path', type=str, default='/content/drive/MyDrive/Savings', help='Inner cycle checkpoint file path for fine tuning')\n",
        "parser.add_argument('--outer_ckpt_path', type=str, default='/content/drive/MyDrive/Savings', help='Outer cycle checkpoint file path for fine tuning')\n",
        "parser.add_argument('--skip_inner', type=bool, default=False, help='Not update inner cycle')\n",
        "parser.add_argument('--use_fid', type=bool, default=False, help='Whether to use fid or not')\n",
        "parser.add_argument('--fid_save_path', type=str, default='/content/drive/MyDrive/Savings', help='Fid save path')\n",
        "parser.add_argument('--fid_s_path', type=str, default='/content/drive/MyDrive/Data/Train', help='Fid source path')\n",
        "parser.add_argument('--fid_t_path', type=str, default='/content/drive/MyDrive/Data/Train', help='Fid target path')\n",
        "parser.add_argument('--fid_bsize', type=int, default=129, help='FID batch size') #128\n",
        "parser.add_argument('--fid_freq', type=int, default=1000, help='FID frequency')\n",
        "parser.add_argument('--best_psnr', type=float, default=25.0, help='Best psnr')\n",
        "parser.add_argument('--ckpt_save_path', type=str, default='/content/drive/MyDrive/checkpoint', help='ckpt save path')\n",
        "parser.add_argument('--use_psnr', type=bool, default=True, help='Whether to use psnr or not')\n",
        "parser.add_argument('--psnr_freq', type=int, default=100, help='PSNR frequency')\n",
        "parser.add_argument('--max_hw', type=int, default=1000, help='Max test input image size')\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "    #return check_args(parser.parse_args())\n",
        "'''\n",
        "def check_args(args):\n",
        "    try:\n",
        "        assert args.iteration >= 1\n",
        "    except:\n",
        "        print('number of iteration must be larger than or equal to one')\n",
        "\n",
        "    try:\n",
        "        assert args.ngpus_per_node == 1\n",
        "    except:\n",
        "        print('Multi-gpu is not supported yet')\n",
        "    return args\n",
        "\n",
        " '''\n",
        "\n",
        "def main_worker_inner(gpu, ngpus_per_node, args):\n",
        "\n",
        "\n",
        "    gan = CinCGAN(gpu, ngpus_per_node, args)\n",
        "    gan.test_batch_size = args.test_bsize\n",
        "    gan.build_model()\n",
        "    gan.train(inner=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "mwSM2uI1NUWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility and Initialization"
      ],
      "metadata": {
        "id": "05a7r2VvCXUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor2pil(t):\n",
        "    img = transforms.functional.to_pil_image(denorm(t))\n",
        "    return img\n",
        "def denorm(t):\n",
        "    return t*0.5 + 0.5\n",
        "\n",
        "def tensor_imsave(t, path, fname, denormalization=True, prt=True):\n",
        "    # Save tensor as .png file\n",
        "    check_folder(path)\n",
        "    if denormalization: t = denorm(t)\n",
        "    t = torch.clamp(input=t, min=0, max=1)\n",
        "    img = transforms.functional.to_pil_image(t.detach().cpu())\n",
        "    img.save(os.path.join(path,fname))\n",
        "    if prt:\n",
        "        print(f\"Saved to {os.path.join(path,fname)}\")\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:##Automatic closing using with. rb = read binary\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "def check_folder(log_dir):\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self,TVLoss_weight=1):\n",
        "        super(TVLoss,self).__init__()\n",
        "        self.TVLoss_weight = TVLoss_weight\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self._tensor_size(x[:,:,1:,:])\n",
        "        count_w = self._tensor_size(x[:,:,:,1:])\n",
        "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
        "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
        "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
        "\n",
        "    def _tensor_size(self,t):\n",
        "        return t.size()[1]*t.size()[2]*t.size()[3]\n",
        "\n",
        "def pil_batch_loader(root_path, bsize):\n",
        "    # Returns the list of PIL.Images at given path. Length of list is bsize.\n",
        "    flist = sorted(glob.glob(root_path, '*.png'))\n",
        "    flist = flist[:bsize]\n",
        "    result = []\n",
        "    for p in flist:\n",
        "        img = pil_loader(p)\n",
        "        result.append(img)\n",
        "    return result\n",
        "\n",
        "class PSNR(object):\n",
        "    def __init__(self, gpu, val_max=1, val_min=0, ycbcr=True):\n",
        "        super(PSNR,self).__init__()\n",
        "        self.val_max = val_max\n",
        "        self.val_min = val_min\n",
        "        self.gpu = gpu\n",
        "        self.ycbcr = ycbcr\n",
        "\n",
        "    def __call__(self,x,y):\n",
        "        \"\"\"\n",
        "        if x.is_cuda:\n",
        "            x = x.detach().cpu()\n",
        "        if y.is_cuda:\n",
        "            y = y.detach().cpu()\n",
        "        \"\"\"\n",
        "        assert len(x.size()) == len(y.size())\n",
        "        with torch.no_grad():\n",
        "            x_lum = rgb_to_ycbcr(x)[:,0]\n",
        "            y_lum = rgb_to_ycbcr(y)[:,0]\n",
        "\n",
        "            mse = torch.mean((y_lum-x_lum)**2, dim=[0,1]) #dim=[1,2]\n",
        "            psnr = 20*torch.log10(torch.tensor(self.val_max-self.val_min, dtype=torch.float).cuda(self.gpu)) - 10*torch.log10(mse)\n",
        "            return torch.mean(psnr)\n",
        "\n",
        "def rgb_to_ycbcr(image):\n",
        "    r\"\"\"Convert an RGB image to YCbCr.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): RGB Image to be converted to YCbCr.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: YCbCr version of the image.\n",
        "    \"\"\"\n",
        "\n",
        "    if not torch.is_tensor(image):\n",
        "        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n",
        "            type(image)))\n",
        "\n",
        "    if len(image.shape) < 3 or image.shape[-3] != 3:\n",
        "        raise ValueError(\"Input size must have a shape of (*, 3, H, W). Got {}\"\n",
        "                         .format(image.shape))\n",
        "\n",
        "    r: torch.Tensor = image[..., 0, :, :]\n",
        "    g: torch.Tensor = image[..., 1, :, :]\n",
        "    b: torch.Tensor = image[..., 2, :, :]\n",
        "\n",
        "    delta = .5\n",
        "    y: torch.Tensor = .299 * r + .587 * g + .114 * b\n",
        "    cb: torch.Tensor = (b - y) * .564 + delta\n",
        "    cr: torch.Tensor = (r - y) * .713 + delta\n",
        "    return torch.stack((y, cb, cr), -3)\n"
      ],
      "metadata": {
        "id": "-Dk3w0WACXkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Data Folder"
      ],
      "metadata": {
        "id": "bBJlpCUlBZeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def has_file_allowed_extension(filename, extensions):\n",
        "    \"\"\"Checks if a file is an allowed extension.\n",
        "\n",
        "    Args:\n",
        "        filename (string): path to a file\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the filename ends with a known image extension\n",
        "    \"\"\"\n",
        "    filename_lower = filename.lower()\n",
        "    return any(filename_lower.endswith(ext) for ext in extensions)\n",
        "\n",
        "def make_dataset(dir, extensions, label):\n",
        "    # Returns the tuples (path, label) of data\n",
        "    # Now, label is same with fname\n",
        "    images = []\n",
        "    for root, _, fnames in sorted(os.walk(dir)):\n",
        "        for fname in sorted(fnames):\n",
        "            if has_file_allowed_extension(fname, extensions):\n",
        "                path = os.path.join(root, fname)\n",
        "                item = (path, fname)\n",
        "                images.append(item)\n",
        "\n",
        "    return images\n",
        "\n",
        "class DatasetFolder(data.Dataset):\n",
        "    def __init__(self, root, label, extensions = ['.jpg', '.png', '.jpeg', '.bmp'], transform=None, return_two_img = False, big_imsize = 129, scale_factor = 3):\n",
        "        # root -- Dataset folder path\n",
        "        # label -- 0 / 1\n",
        "        # extensions -- list of alowed extensions\n",
        "        # transform -- Transform applied to image\n",
        "        # return_two_img -- Whether to return both big and small imgs or not\n",
        "        # big_imsize -- Size of bigger img. Matters only if return_two_img is True\n",
        "        # scale_factor -- SR scale factor. Matters only if return_two_img is True\n",
        "        samples = make_dataset(root, extensions, label)\n",
        "\n",
        "        if len(samples) == 0:\n",
        "            raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n",
        "                               \"Supported extensions are: \" + \",\".join(extensions)))\n",
        "\n",
        "        self.root = root\n",
        "        self.extensions = extensions\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "        self.return_two_img = return_two_img\n",
        "        self.big_imsize = big_imsize\n",
        "        self.scale_factor = scale_factor\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (sample, target) where target is class_index of the target class.\n",
        "        \"\"\"\n",
        "\n",
        "        path, target = self.samples[index]\n",
        "        sample = pil_loader(path)\n",
        "\n",
        "\n",
        "\n",
        "        if self.return_two_img:\n",
        "            t1 = transforms.Compose([\n",
        "            Random90Rot(),\n",
        "            transforms.RandomCrop((self.big_imsize,self.big_imsize))])\n",
        "\n",
        "            t2 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
        "\n",
        "            sample = t1(sample)\n",
        "            sample_big = t2(sample)\n",
        "            sample_small = transforms.Resize((self.big_imsize//self.scale_factor, self.big_imsize//self.scale_factor), Image.BICUBIC)(sample)\n",
        "            sample_small = t2(sample_small)\n",
        "\n",
        "            return sample_big, sample_small, target\n",
        "\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "metadata": {
        "id": "HcFIdOPyBaOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator and Discriminator Networks"
      ],
      "metadata": {
        "id": "zyJnFdHdGjQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetGenerator(nn.Module):\n",
        "    #Generator architecture\n",
        "    def __init__(self, input_nc=3, output_nc=3, inter_nc=64, n_blocks=6, img_size = 32, use_bias = False, rs_norm = 'BN', padding_type = 'zero', dsple = False, scale_factor=4):\n",
        "        # input_nc(int) -- The number of channels of input img\n",
        "        # output_nc(int) -- The number of channels of output img\n",
        "        # inter_nc(int) -- The number of filters of intermediate layers\n",
        "        # n_blocks(int) -- The number of resnet blocks\n",
        "        # img_size(int) -- Input image size\n",
        "        # use_bias(bool) -- Whether to use bias on conv layer or not\n",
        "        # rs_norm(str) -- The type of normalization method of ResnetBlock. BN : Batch Normalization, IN : Instance Normalization, else : none\n",
        "        # padding_type(str) -- The name of padding layer: reflect | replicate | zero\n",
        "        # dsple(bool) -- Whether to downsample or maintain input image. Set it true for G3.\n",
        "        # scale_factor(int) -- Scale factor, 2 / 4\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "\n",
        "        self.input_nc = input_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.inter_nc = inter_nc\n",
        "        self.n_blocks = n_blocks\n",
        "        self.img_size = img_size\n",
        "        self.use_bias = use_bias\n",
        "        self.rs_norm = rs_norm\n",
        "        self.padding_type = padding_type\n",
        "        self.dsple = dsple\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "        # Input blocks\n",
        "        InBlock = []\n",
        "\n",
        "        InBlock += [nn.Conv2d(input_nc, inter_nc, kernel_size=7, stride=1, padding=3, bias=self.use_bias),\n",
        "                     nn.LeakyReLU(0.2)]\n",
        "        InBlock += [nn.Conv2d(inter_nc, inter_nc, kernel_size=3, stride=2 if self.dsple and self.scale_factor==4 else 1, padding=1, bias=self.use_bias),\n",
        "                     nn.LeakyReLU(0.2)] #changed\n",
        "        InBlock += [nn.Conv2d(inter_nc, inter_nc, kernel_size=3, stride=2 if self.dsple else 1, padding=1, bias=self.use_bias),\n",
        "                     nn.LeakyReLU(0.2)]\n",
        "\n",
        "        # ResnetBlocks\n",
        "        ResnetBlocks = []\n",
        "\n",
        "        for i in range(n_blocks):\n",
        "            ResnetBlocks += [ResnetBlock(inter_nc, self.padding_type, self.rs_norm, self.use_bias)]\n",
        "\n",
        "        # Output block\n",
        "        OutBlock = []\n",
        "\n",
        "        OutBlock += [nn.Conv2d(inter_nc, inter_nc, kernel_size=3, stride=1, padding=1, bias=self.use_bias),\n",
        "                     nn.LeakyReLU(0.2)]\n",
        "        OutBlock += [nn.Conv2d(inter_nc, inter_nc, kernel_size=3, stride=1, padding=1, bias=self.use_bias),\n",
        "                     nn.LeakyReLU(0.2)]\n",
        "        OutBlock += [nn.Conv2d(inter_nc, output_nc, kernel_size=7, stride=1, padding=3, bias=self.use_bias),\n",
        "                     nn.LeakyReLU(0.2)]\n",
        "\n",
        "        self.InBlock = nn.Sequential(*InBlock)\n",
        "        self.ResnetBlocks = nn.Sequential(*ResnetBlocks)\n",
        "        self.OutBlock = nn.Sequential(*OutBlock)\n",
        "    def forward(self,x):\n",
        "        out = self.InBlock(x)\n",
        "        out = self.ResnetBlocks(out)\n",
        "        out = self.OutBlock(out)\n",
        "\n",
        "        return out\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type, norm_type, use_bias):\n",
        "        # dim(int) -- The number of channels in the resnet blocks\n",
        "        # padding_type(str) -- The name of padding layer: reflect | replicate | zero\n",
        "        # norm_type(str) -- The type of normalization method. BN : Batch Normalization, IN : Instance Normalization, else : none\n",
        "        # use_bias -- Whether to use bias on conv layer or not\n",
        "        super(ResnetBlock, self).__init__()\n",
        "\n",
        "        conv_block = []\n",
        "\n",
        "        # Padding\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        if norm_type=='BN':\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        elif norm_type=='IN':\n",
        "            norm_layer = nn.InstanceNorm2d\n",
        "        else:\n",
        "            raise NotImplementedError('Normalization [%s] is not implemented' % norm_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.LeakyReLU(0.2)]\n",
        "\n",
        "\n",
        "        # Padding\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.LeakyReLU(0.2)]\n",
        "\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "\n",
        "        # Skip connection\n",
        "        out = out + x\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc=3, norm_type = 'BN', use_bias = True, is_inner=True, scale_factor=4):\n",
        "        # input_nc(int) -- The number of channels of input img\n",
        "        # norm_type(str) -- The type of normalization method. BN : Batch Normalization, IN : Instance Normalization, else : none\n",
        "        # use_bias(bool) -- Whether to use bias or not\n",
        "        # is_inner(bool) -- True : For inner cycle, False : For outer cycle\n",
        "        # scale_factor(int) -- Scale factor, 2 / 4\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        if norm_type=='BN':\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "            use_bias = False # There is no need to use bias because BN already has shift parameter.\n",
        "        elif norm_type=='IN':\n",
        "            norm_layer = nn.InstanceNorm2d\n",
        "        else:\n",
        "            raise NotImplementedError('Normalization [%s] is not implemented' % norm_type)\n",
        "\n",
        "        if is_inner == True:\n",
        "            s = 1\n",
        "        elif is_inner == False:\n",
        "            s = 2\n",
        "        else:\n",
        "            raise NotImplementedError('is_inner must be boolean.')\n",
        "\n",
        "        nfil_mul = 64\n",
        "        p=0 # Why 1???\n",
        "        layers = []\n",
        "        layers += [nn.Conv2d(input_nc, nfil_mul, kernel_size=4, stride = 2 if is_inner==True and scale_factor==2 else s, padding=p, bias=use_bias),\n",
        "                       nn.LeakyReLU(0.2)] # changed\n",
        "        layers += [nn.Conv2d(nfil_mul, nfil_mul*2, kernel_size=4, stride = s, padding=p, bias=use_bias),\n",
        "                       norm_layer(nfil_mul*2),\n",
        "                       nn.LeakyReLU(0.2)]\n",
        "        layers += [nn.Conv2d(nfil_mul*2, nfil_mul*4, kernel_size=4, stride = s, padding=p, bias=use_bias),\n",
        "                       norm_layer(nfil_mul*4),\n",
        "                       nn.LeakyReLU(0.2)]\n",
        "        layers += [nn.Conv2d(nfil_mul*4, nfil_mul*8, kernel_size=4, stride = 1, padding=p, bias=use_bias),\n",
        "                       norm_layer(nfil_mul*8),\n",
        "                       nn.LeakyReLU(0.2)]\n",
        "        layers += [nn.Conv2d(nfil_mul*8, 1, kernel_size=4, stride = 1, padding=p, bias=use_bias),\n",
        "                       nn.LeakyReLU(0.2)]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "\n",
        "        return out # Predicted values of each patches"
      ],
      "metadata": {
        "id": "3ZXuv-LRFqOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inner Cycle Model"
      ],
      "metadata": {
        "id": "p3MxHFzDIbV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Crop(object):\n",
        "    def __init__(self, max_hw):\n",
        "        self.max_hw = max_hw\n",
        "    def __call__(self,x):\n",
        "        if x.size[0]>=self.max_hw and x.size[1]>=self.max_hw:\n",
        "            x = transforms.CenterCrop((self.max_hw,self.max_hw))(x)\n",
        "        return x\n",
        "\n",
        "class Random90Rot(object):\n",
        "    def __call__(self, img):\n",
        "        angle_list = [0,-90,-180,90]\n",
        "        angle = random.choice(angle_list)\n",
        "\n",
        "        return transforms.functional.rotate(img, angle)"
      ],
      "metadata": {
        "id": "mds6aJIWvy7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Computing mean and std of dataset\n",
        "mean = 0.0\n",
        "meansq = 0.0\n",
        "count = 0\n",
        "\n",
        "for index, data in enumerate(train_loader):\n",
        "    mean = data.sum()\n",
        "    meansq = meansq + (data**2).sum()\n",
        "    count += np.prod(data.shape)\n",
        "\n",
        "total_mean = mean/count\n",
        "total_var = (meansq/count) - (total_mean**2)\n",
        "total_std = torch.sqrt(total_var)\n",
        "print(\"mean: \" + str(total_mean))\n",
        "print(\"std: \" + str(total_std))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "HMH2ezDh4uLm",
        "outputId": "6ecd5e8b-767f-48ce-cd5a-c7963e37c60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nComputing mean and std of dataset\\nmean = 0.0\\nmeansq = 0.0\\ncount = 0\\n\\nfor index, data in enumerate(train_loader):\\n    mean = data.sum()\\n    meansq = meansq + (data**2).sum()\\n    count += np.prod(data.shape)\\n\\ntotal_mean = mean/count\\ntotal_var = (meansq/count) - (total_mean**2)\\ntotal_std = torch.sqrt(total_var)\\nprint(\"mean: \" + str(total_mean))\\nprint(\"std: \" + str(total_std))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CinCGAN():\n",
        "    def __init__(self, gpu, ngpus_per_node, args):\n",
        "        # Device configuration\n",
        "\n",
        "        torch.cuda.set_device(gpu)\n",
        "\n",
        "        # self.device = torch.device('cuda:'+ args.gpu if torch.cuda.is_available() else 'cpu')\n",
        "        self.device = torch.device('cpu')\n",
        "\n",
        "        print(\"GPU : \", gpu)\n",
        "        self.lr = args.inner_lr\n",
        "        self.decay_step = args.decay_step\n",
        "        self.batch_size = args.bsize//ngpus_per_node\n",
        "        self.test_batch_size = args.test_bsize\n",
        "        print(f\"GPU : {gpu}, BatchSize : {self.batch_size}\")\n",
        "        self.iteration = args.iteration\n",
        "        self.eps = args.eps\n",
        "        self.w1 = args.w1\n",
        "        self.w2 = args.w2\n",
        "        self.w3 = args.w3\n",
        "\n",
        "        self.lr_outer = args.outer_lr\n",
        "\n",
        "        self.gamma0 = args.gamma0\n",
        "        self.gamma1 = args.gamma1\n",
        "        self.gamma2 = args.gamma2\n",
        "        self.gamma3 = args.gamma3\n",
        "        self.gamma4 = args.gamma4\n",
        "\n",
        "        self.train_s_path = args.train_s_path\n",
        "        self.train_t_path = args.train_t_path\n",
        "        self.test_s_path = args.test_s_path\n",
        "        self.test_t_path = args.test_t_path\n",
        "        self.fid_s_path = self.test_s_path\n",
        "        self.gpu = gpu\n",
        "        self.ngpus_per_node = ngpus_per_node\n",
        "        self.num_workers = args.num_workers\n",
        "\n",
        "        self.step = 0\n",
        "\n",
        "        self.resume = args.resume\n",
        "        self.resume_iter = args.resume_iter\n",
        "        self.inner_ckpt_path = args.inner_ckpt_path\n",
        "        self.neptune = args.neptune\n",
        "        self.params = vars(args)\n",
        "\n",
        "\n",
        "\n",
        "        self.save_freq = args.save_freq\n",
        "\n",
        "        self.scale_factor = args.scale_factor\n",
        "        self.skip_inner = args.skip_inner\n",
        "        self.use_fid = args.use_fid\n",
        "        self.fid_save_path = args.fid_save_path\n",
        "        #self.fid_s_path = args.fid_s_path\n",
        "        #self.fid_t_path = args.fid_t_path\n",
        "        #self.fid_bsize = args.fid_bsize\n",
        "        #self.fid_freq = args.fid_freq\n",
        "        #self.fid_mean = None\n",
        "        #self.fid_cov = None\n",
        "        self.use_psnr = args.use_psnr\n",
        "        self.psnr_freq = args.psnr_freq\n",
        "        self.ckpt_save_path = args.ckpt_save_path\n",
        "        if os.path.isdir('/mnt/workspace/CinCGAN-pytorch/CinCGAN_pytorch/target_fid_statistics'):\n",
        "            self.fid_mean = np.load(os.path.join('/mnt/workspace/CinCGAN-pytorch/CinCGAN_pytorch/target_fid_statistics', 'm2.npy'))\n",
        "            self.fid_cov = np.load(os.path.join('/mnt/workspace/CinCGAN-pytorch/CinCGAN_pytorch/target_fid_statistics', 's2.npy'))\n",
        "\n",
        "\n",
        "        self.inception = None\n",
        "        self.best_psnr = args.best_psnr\n",
        "\n",
        "        self.imsize_y = 129 ##128\n",
        "        assert self.imsize_y % self.scale_factor == 0, 'invalid scale factor'\n",
        "        self.imsize_x = 128//self.scale_factor\n",
        "        self.max_hw = args.max_hw\n",
        "        print(f\"NEPTUNE : {self.neptune}\")\n",
        "        print(f\"gamma0 : {self.gamma0}\")\n",
        "        print(f\"gamma1 : {self.gamma1}\")\n",
        "        print(f\"gamma2 : {self.gamma2}\")\n",
        "        print(f\"gamma3 : {self.gamma3}\")\n",
        "        print(f\"gamma4 : {self.gamma4}\")\n",
        "        print(f\"train_s_path : {self.train_s_path}\")\n",
        "        print(f\"train_t_path : {self.train_t_path}\")\n",
        "\n",
        "        print(f\"test_s_path : {self.test_s_path}\")\n",
        "        print(f\"test_t_path : {self.test_t_path}\")\n",
        "        print(f\"scale factor : {self.scale_factor}\")\n",
        "\n",
        "    def psnr_mean(self, inner, num=10):\n",
        "        # Derive mean of psnr of whole test data\n",
        "        # num : int, number of samples\n",
        "        test_s_iter = iter(self.test_s_loader)\n",
        "        test_t_iter = iter(self.test_t_loader)\n",
        "        num = np.minimum(num, len(self.test_s_loader))\n",
        "        self.G1_forward.eval()\n",
        "        psnr_mean = 0\n",
        "        i=0\n",
        "        with torch.no_grad():\n",
        "            while True:\n",
        "                try:\n",
        "                    x, _ = test_s_iter.next()\n",
        "                except:\n",
        "                    break\n",
        "                try:\n",
        "                    y_big, _ = test_t_iter.next()\n",
        "                except:\n",
        "                    break\n",
        "                y = transforms.Resize(x.size()[-2:], interpolation=Image.BICUBIC)(tensor2pil(y_big[0])) # To resize, tensor should be converted to pillow image.\n",
        "                y = transforms.ToTensor()(y)\n",
        "                y = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(y)\n",
        "                y = torch.unsqueeze(y,0)\n",
        "                y = y.to(self.device)\n",
        "\n",
        "                x, y_big = x.to(self.device), y_big.to(self.device)\n",
        "                # fake_inner = self.G1_forward(x)\n",
        "                out = self.G1_forward(x)\n",
        "                if inner == True:\n",
        "                    out = out * 0.5 + 0.5\n",
        "                    psnr = self.PSNR(y*0.5 + 0.5, out)\n",
        "                psnr_mean += psnr/num\n",
        "                print(f\"i : {i}, psnr : {psnr:0.4f}\")\n",
        "                i+=1\n",
        "                if i==num:\n",
        "                    break\n",
        "        return psnr_mean\n",
        "\n",
        "    def save(self, is_outer=True, fname = 'default'):\n",
        "        if fname == 'default':\n",
        "            fname = f'{self.step}.pt'\n",
        "        check_folder(os.path.join(self.ckpt_save_path, 'ckpt'))\n",
        "        params = {}\n",
        "        params['G1_forward'] = self.G1_forward.state_dict()\n",
        "        params['G1_backward'] = self.G1_backward.state_dict()\n",
        "        params['D1_forward'] = self.D1_forward.state_dict()\n",
        "        params['D1_backward'] = self.D1_backward.state_dict()\n",
        "\n",
        "        torch.save(params, os.path.join(self.ckpt_save_path, 'ckpt', fname))\n",
        "        if is_outer==True:\n",
        "            check_folder(os.path.join(self.ckpt_save_path, 'ckpt_edsr'))\n",
        "            params_edsr = {}\n",
        "            params_edsr['EDSR'] = self.EDSR.state_dict()\n",
        "            torch.save(params_edsr, os.path.join(self.ckpt_save_path,'ckpt_edsr', fname))\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "\n",
        "        \"\"\" Define Inner Cycle Models \"\"\"\n",
        "\n",
        "\n",
        "        self.G1_forward = ResnetGenerator(scale_factor = self.scale_factor).to(self.device)\n",
        "        self.G1_backward = ResnetGenerator(scale_factor = self.scale_factor).to(self.device)\n",
        "        self.D1_forward = Discriminator(scale_factor = self.scale_factor).to(self.device)\n",
        "        self.D1_backward = Discriminator(scale_factor = self.scale_factor).to(self.device)\n",
        "\n",
        "        \"\"\" Define Loss \"\"\"\n",
        "        self.MSE_loss = nn.MSELoss().to(self.device)\n",
        "        self.TV_loss= TVLoss().to(self.device)\n",
        "\n",
        "        self.PSNR = PSNR(self.gpu)\n",
        "\n",
        "        \"\"\" Optimizers \"\"\"\n",
        "        self.G_optim = torch.optim.Adam(itertools.chain(self.G1_forward.parameters(), self.G1_backward.parameters()),lr=self.lr, betas=(0.5,0.999), eps = self.eps)\n",
        "        self.D_optim = torch.optim.Adam(itertools.chain(self.D1_forward.parameters(), self.D1_backward.parameters()),lr=self.lr, betas=(0.5,0.999), eps = self.eps)\n",
        "\n",
        "        \"\"\" Data loader \"\"\"\n",
        "\n",
        "        train_transform_s = transforms.Compose([\n",
        "            Random90Rot(),\n",
        "            transforms.RandomCrop((self.imsize_x,self.imsize_x)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        train_transform_t = transforms.Compose([\n",
        "            Random90Rot(),\n",
        "            transforms.RandomCrop((self.imsize_y,self.imsize_y)),\n",
        "            transforms.Resize((self.imsize_x,self.imsize_x), Image.BICUBIC), ###\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "        test_transform = transforms.Compose([\n",
        "            Crop(self.max_hw),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.train_s_dataset = DatasetFolder(self.train_s_path, 0, transform = train_transform_s)\n",
        "        self.train_t_dataset = DatasetFolder(self.train_t_path, 1, transform=None, return_two_img = True, big_imsize=self.imsize_y, scale_factor = self.scale_factor)\n",
        "\n",
        "        self.test_s_dataset = DatasetFolder(self.test_s_path, 0, transform=test_transform)\n",
        "        self.test_t_dataset = DatasetFolder(self.test_t_path, 1, transform=test_transform)\n",
        "\n",
        "        self.train_s_loader = DataLoader(self.train_s_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "        self.train_t_loader = DataLoader(self.train_t_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "        self.test_s_loader = DataLoader(self.test_s_dataset, batch_size=self.test_batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "        self.test_t_loader = DataLoader(self.test_t_dataset, batch_size=self.test_batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def train(self, inner=True):\n",
        "        psnr = 0\n",
        "        psnr_inner = 0\n",
        "        check_g3 = 0\n",
        "        fid_inner = 0\n",
        "        fid_outer = 0\n",
        "        self.inner = inner\n",
        "        # torch.autograd.set_detect_anomaly(True)\n",
        "        for model in [self.G1_forward, self.G1_backward, self.D1_forward, self.D1_backward]:\n",
        "            model.train()\n",
        "        if self.skip_inner:\n",
        "            self.G1_forward.eval()\n",
        "        start_iter = 0\n",
        "        print(f\"NEPTUNE : {self.neptune}\")\n",
        "        if self.neptune:\n",
        "            neptune.init(project_qualified_name='ml.swlee/CinCGAN',\n",
        "                        api_token = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiYmVhNmFlOGMtZDRmZS00NzIyLWJkYzgtNTcyZTk0ZTM5YzM1In0=')\n",
        "            neptune.create_experiment(params = self.params)\n",
        "        if self.resume:\n",
        "            start_iter = self.resume_iter\n",
        "            if inner:\n",
        "                self.load(path_inner = self.inner_ckpt_path)\n",
        "\n",
        "        # Validation data\n",
        "        dataiter = iter(self.test_s_loader)\n",
        "        x_test, _  = next(dataiter)\n",
        "        #x_test, _ = iter(self.test_s_loader).next()\n",
        "        #x_test = x_test.to(self.device)\n",
        "\n",
        "        dataiter1 = iter(self.test_t_loader)\n",
        "        y_test_big, _  = next(dataiter1)\n",
        "        #y_test_big, _ = iter(self.test_t_loader).next()\n",
        "        # if self.use_fid == False:\n",
        "        #     assert x_test.size()[0] == y_test_big.size()[0] == 1\n",
        "        y_test = transforms.Resize(x_test.size()[-2:], interpolation=Image.BICUBIC)(tensor2pil(y_test_big[0])) # To resize, tensor should be converted to pillow image.\n",
        "        y_test = transforms.ToTensor()(y_test)\n",
        "        y_test = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(y_test)\n",
        "        y_test = torch.unsqueeze(y_test,0)\n",
        "        y_test = y_test.to(self.device)\n",
        "        y_test_big = y_test_big.to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "        if self.use_psnr: base_d = self.PSNR(denorm(x_test[0]), denorm(y_test[0])) ###\n",
        "        if self.ngpus_per_node>1:\n",
        "            if self.gpu==0:\n",
        "                x_test, _ = iter(self.test_s_loader).next()\n",
        "                x_test = x_test.to(self.device)\n",
        "\n",
        "                y_test_big, _ = iter(self.test_t_loader).next()\n",
        "                assert x_test.size()[0] == y_test_big.size()[0] == 1\n",
        "                y_test = transforms.Resize(x_test.size()[-2:], interpolation=Image.BICUBIC)(tensor2pil(y_test_big[0])) # To resize, tensor should be converted to pillow image.\n",
        "                y_test = torch.unsqueeze(transforms.ToTensor()(y_test),0)\n",
        "                y_test = y_test.to(self.device)\n",
        "\n",
        "\n",
        "                if self.use_psnr: base_d = self.PSNR(x_test*0.5 + 0.5, y_test*0.5 + 0.5)\n",
        "\n",
        "\n",
        "\n",
        "        if self.skip_inner:\n",
        "            freeze_model(self.G1_forward)\n",
        "            print(\"----------------------------Model Freezed\")\n",
        "\n",
        "\n",
        "        for step in range(start_iter, self.iteration+1):\n",
        "            self.step = step\n",
        "            t = time.time()\n",
        "            if step%self.decay_step==0:\n",
        "                n = step//self.decay_step\n",
        "                self.G_optim.param_groups[0]['lr'] = self.lr/2**n # param_groups = [{'amsgrad': False, 'betas': (...), 'eps': 1e-08, 'lr': 0.0002, 'params': [...], 'weight_decay': 0}]\n",
        "                self.D_optim.param_groups[0]['lr'] = self.lr/2**n\n",
        "                #self.G_outer_optim.param_groups[0]['lr'] = self.lr_outer/2**n\n",
        "                #self.D2_optim.param_groups[0]['lr'] = self.lr_outer/2**n\n",
        "\n",
        "\n",
        "            dataiter2 = iter(self.train_s_loader)\n",
        "            x, label_x  = next(dataiter2)\n",
        "\n",
        "            dataiter3 = iter(self.train_t_loader)\n",
        "            y_big, y, label_y  = next(dataiter3)\n",
        "            '''\n",
        "            try:\n",
        "                x, label_x = train_s_iter.next()\n",
        "            except:\n",
        "                train_s_iter = iter(self.train_s_loader)\n",
        "                x, label_x = train_s_iter.next()\n",
        "\n",
        "            try:\n",
        "                y_big, y, label_y = train_t_iter.next()\n",
        "\n",
        "            except:\n",
        "                train_t_iter = iter(self.train_t_loader)\n",
        "                y_big, y, label_y = train_t_iter.next()\n",
        "\n",
        "            '''\n",
        "            if self.ngpus_per_node>1:\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "\n",
        "            y_big = y_big.to(self.device)\n",
        "            print(f\"x.shape : {x.size()}, y.shape : {y.size()}, y_big.shape : {y_big.size()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if self.gpu==0: print(f\"Data loading : {time.time()-t:.3f}\")\n",
        "            t = time.time()\n",
        "            # Update discriminator\n",
        "            self.D_optim.zero_grad()\n",
        "            if self.skip_inner==False:\n",
        "\n",
        "\n",
        "                fake_forward = self.G1_forward(x)\n",
        "                fake_backward = self.G1_backward(y)\n",
        "                print(f\"fake_forward.shape : {fake_forward.size()} fake_backward.shape : {fake_backward.size()} \")\n",
        "                # D1 output for real images\n",
        "                dis1_forward_output_real = self.D1_forward(y)\n",
        "                dis1_backward_output_real = self.D1_backward(x)\n",
        "\n",
        "                # D1 output for fake images\n",
        "                dis1_forward_output_fake = self.D1_forward(fake_forward)\n",
        "                dis1_backward_output_fake = self.D1_backward(fake_backward)\n",
        "                print(f\"dis1_forward_output_fake.shape : {dis1_forward_output_fake.size()} dis1_backward_output_fake.shape : {dis1_backward_output_fake.size()} \")\n",
        "                # D1 prediction for real images\n",
        "                dis1_forward_predicted_real = torch.mean(dis1_forward_output_real, dim = [1,2,3])\n",
        "                dis1_backward_predicted_real = torch.mean(dis1_backward_output_real, dim = [1,2,3])\n",
        "\n",
        "                # D1 prediction for fake images\n",
        "                dis1_forward_predicted_fake = torch.mean(dis1_forward_output_fake, dim = [1,2,3])\n",
        "                dis1_backward_predicted_fake = torch.mean(dis1_backward_output_fake, dim = [1,2,3])\n",
        "\n",
        "\n",
        "\n",
        "                # Discriminator loss\n",
        "                dis1_forward_ad_loss = self.MSE_loss(dis1_forward_predicted_fake, torch.zeros_like(dis1_forward_predicted_fake).to(self.device)) + self.MSE_loss(dis1_forward_predicted_real, torch.ones_like(dis1_forward_predicted_real).to(self.device))\n",
        "                dis1_backward_ad_loss = self.MSE_loss(dis1_backward_predicted_fake, torch.zeros_like(dis1_backward_predicted_fake).to(self.device)) + self.MSE_loss(dis1_backward_predicted_real, torch.ones_like(dis1_backward_predicted_real).to(self.device))\n",
        "\n",
        "                dis1_loss = dis1_forward_ad_loss + dis1_backward_ad_loss\n",
        "\n",
        "                dis1_loss.backward()\n",
        "                self.D_optim.step()\n",
        "\n",
        "\n",
        "\n",
        "                # Update Generator\n",
        "                self.G_optim.zero_grad()\n",
        "\n",
        "                fake_forward = self.G1_forward(x)\n",
        "                fake_backward = self.G1_backward(y)\n",
        "\n",
        "                # D1 output for fake images\n",
        "                dis1_forward_output_fake = self.D1_forward(fake_forward)\n",
        "                dis1_backward_output_fake = self.D1_backward(fake_backward)\n",
        "\n",
        "                # D1 prediction for fake images\n",
        "                dis1_forward_predicted_fake = torch.mean(dis1_forward_output_fake, dim = [1,2,3])\n",
        "                dis1_backward_predicted_fake = torch.mean(dis1_backward_output_fake, dim = [1,2,3])\n",
        "\n",
        "                # Adversarial loss\n",
        "                g1_forward_ad_loss = self.MSE_loss(dis1_forward_predicted_fake, torch.ones_like(dis1_forward_predicted_fake).to(self.device))\n",
        "                g1_backward_ad_loss = self.MSE_loss(dis1_backward_predicted_fake, torch.ones_like(dis1_backward_predicted_fake).to(self.device))\n",
        "\n",
        "                # Cycle consistency loss\n",
        "                g1_forward_cycle_loss = self.MSE_loss(self.G1_backward(fake_forward), x)\n",
        "                g1_backward_cycle_loss = self.MSE_loss(self.G1_forward(fake_backward), y)\n",
        "\n",
        "                # Identity loss\n",
        "                g1_forward_identity_loss = self.MSE_loss(self.G1_forward(y), y)\n",
        "                g1_backward_identity_loss = self.MSE_loss(self.G1_backward(x), x)\n",
        "\n",
        "                # Total variation loss\n",
        "                g1_forward_tv_loss = self.TV_loss(fake_forward)\n",
        "\n",
        "                if inner==False:\n",
        "                    self.w2 = 1\n",
        "\n",
        "                g1_forward_loss = g1_forward_ad_loss + self.w1*g1_forward_cycle_loss + self.w2*g1_forward_identity_loss + self.w3*g1_forward_tv_loss\n",
        "                g1_backward_loss = g1_backward_ad_loss + self.w1*g1_backward_cycle_loss + self.w2*g1_backward_identity_loss\n",
        "\n",
        "                g1_loss = g1_forward_loss + g1_backward_loss\n",
        "\n",
        "                g1_loss.backward()\n",
        "                self.G_optim.step()\n",
        "\n",
        "                if inner==True:\n",
        "                    print(f\"endloop : {time.time()-t:.3f}\")\n",
        "                    t = time.time()\n",
        "\n",
        "                    print(f\"step : {step}  d_loss : {dis1_loss}  g_loss : {g1_loss}  lr : {self.D_optim.param_groups[0]['lr']}\")\n",
        "            # get inner psnr\n",
        "\n",
        "            self.G1_forward.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                img_t = self.G1_forward(x_test)[0]\n",
        "                if self.use_psnr and step%self.psnr_freq==0:\n",
        "                    #psnr = self.PSNR(img_t*0.5 + 0.5, y_test[0]*0.5 + 0.5)\n",
        "                    psnr_inner = self.psnr_mean(inner=True)\n",
        "                    print(f\"inner psnr : {psnr_inner}\")\n",
        "                    if inner:\n",
        "                        if psnr_inner > self.best_psnr:\n",
        "                            self.best_psnr = psnr_inner\n",
        "                            self.save(is_outer = False, fname = f'inner_{step}_{psnr_inner.item():0.4f}.pt')\n",
        "            '''\n",
        "            if inner==True and self.use_fid==True:\n",
        "                if step%self.fid_freq==0:\n",
        "                    fid_inner = self.fid()\n",
        "'''\n",
        "            if self.skip_inner == False:\n",
        "                self.G1_forward.train()\n",
        "            if self.neptune:\n",
        "                if self.use_psnr: neptune.log_metric('inner_psnr', psnr_inner)\n",
        "                if self.use_fid: neptune.log_metric('inner_fid', fid_inner)\n",
        "\n",
        "            if inner==True:\n",
        "                if step%50==0:\n",
        "\n",
        "                    tensor_imsave(img_t, f'/mnt/workspace/CinCGAN-pytorch/CinCGAN_pytorch/imgs', f'img{step}.png')\n",
        "                if self.skip_inner == False: self.G1_forward.train()\n",
        "\n",
        "            if inner==True:\n",
        "                if step%self.save_freq == 0:\n",
        "                    self.save(is_outer=False)\n",
        "                continue\n",
        ""
      ],
      "metadata": {
        "id": "YwHBGkx1GFCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    #args = parse_args()\n",
        "\n",
        "    gpu = args.gpu\n",
        "    ngpus_per_node = args.ngpus_per_node\n",
        "\n",
        "    #os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu\n",
        "    phase =args.phase\n",
        "    if phase == 'train_inner' or phase == 'train_outer':\n",
        "\n",
        "        main_worker = main_worker_inner if phase=='train_inner' else main_worker_outer\n",
        "        # ngpus_per_node = torch.cuda.device_count()\n",
        "        world_size = ngpus_per_node\n",
        "        print(\"Available gpu num : \", ngpus_per_node)\n",
        "\n",
        "        if ngpus_per_node >1:\n",
        "            torch.multiprocessing.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, ))\n",
        "        else:\n",
        "            main_worker(int(gpu), 1, args)\n",
        "\n",
        "\n",
        "    elif phase == 'test_inner' or phase == 'test_outer':\n",
        "\n",
        "        print('----------Test mode----------')\n",
        "        gan = CinCGAN(int(gpu), 1, args)\n",
        "        gan.build_model()\n",
        "        gan.load(args.inner_ckpt_path, inner= phase=='test_inner', path_outer=args.outer_ckpt_path)\n",
        "        for i in range(18):\n",
        "            gan.test(inner= phase=='test_inner', idx=i)\n",
        "        i = 0\n",
        "        # print(gan.psnr_mean(inner=False, num=800))\n",
        "        # result = gan.psnr_mean()\n",
        "        # print(f\"Mean PSNR : {result}\")\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56GoFLpGY4hP",
        "outputId": "d2ffeff1-4326-4a5b-e28f-44f5db5774dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available gpu num :  1\n",
            "GPU :  0\n",
            "GPU : 0, BatchSize : 16\n",
            "NEPTUNE : False\n",
            "gamma0 : 1.0\n",
            "gamma1 : 10.0\n",
            "gamma2 : 5.0\n",
            "gamma3 : 2.0\n",
            "gamma4 : 0.0\n",
            "train_s_path : /content/drive/MyDrive/Data/Train\n",
            "train_t_path : /content/drive/MyDrive/Data/Validation\n",
            "test_s_path : /content/drive/MyDrive/Test /Test_source\n",
            "test_t_path : /content/drive/MyDrive/Test /Test_target\n",
            "scale factor : 3\n",
            "NEPTUNE : False\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.272\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.995\n",
            "step : 0  d_loss : 1.4340181350708008  g_loss : 10.830108642578125  lr : 0.0002\n",
            "inner psnr : 0\n",
            "Saved to /mnt/workspace/CinCGAN-pytorch/CinCGAN_pytorch/imgs/img0.png\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.268\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.836\n",
            "step : 1  d_loss : 13.11448860168457  g_loss : 12.04284954071045  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.250\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 26.352\n",
            "step : 2  d_loss : 2.3836259841918945  g_loss : 8.750434875488281  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.251\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 26.112\n",
            "step : 3  d_loss : 2.2057647705078125  g_loss : 8.786492347717285  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.257\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.320\n",
            "step : 4  d_loss : 1.459815502166748  g_loss : 8.223163604736328  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.252\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.282\n",
            "step : 5  d_loss : 1.5582064390182495  g_loss : 5.403810977935791  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.251\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.308\n",
            "step : 6  d_loss : 1.37050461769104  g_loss : 4.969141960144043  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.259\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.585\n",
            "step : 7  d_loss : 1.1979153156280518  g_loss : 4.34317684173584  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.263\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.659\n",
            "step : 8  d_loss : 1.2517409324645996  g_loss : 3.8083410263061523  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.258\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.665\n",
            "step : 9  d_loss : 1.3705661296844482  g_loss : 4.039134979248047  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.261\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.871\n",
            "step : 10  d_loss : 0.8988974094390869  g_loss : 3.819366455078125  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.254\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.577\n",
            "step : 11  d_loss : 0.9436057806015015  g_loss : 3.4256176948547363  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.258\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.793\n",
            "step : 12  d_loss : 1.0819084644317627  g_loss : 3.576591968536377  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.248\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.651\n",
            "step : 13  d_loss : 0.983984112739563  g_loss : 3.391239643096924  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.257\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.546\n",
            "step : 14  d_loss : 0.8830642700195312  g_loss : 3.6774821281433105  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.257\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.528\n",
            "step : 15  d_loss : 0.7919520139694214  g_loss : 3.633975028991699  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.255\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.689\n",
            "step : 16  d_loss : 0.7821081280708313  g_loss : 3.271336078643799  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.260\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.085\n",
            "step : 17  d_loss : 0.6869615912437439  g_loss : 3.371750831604004  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.256\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.502\n",
            "step : 18  d_loss : 0.7524799108505249  g_loss : 2.9333410263061523  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.261\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.036\n",
            "step : 19  d_loss : 0.6262578964233398  g_loss : 3.398777961730957  lr : 0.0002\n",
            "x.shape : torch.Size([16, 3, 42, 42]), y.shape : torch.Size([16, 3, 43, 43]), y_big.shape : torch.Size([16, 3, 129, 129])\n",
            "Data loading : 0.264\n",
            "fake_forward.shape : torch.Size([16, 3, 42, 42]) fake_backward.shape : torch.Size([16, 3, 43, 43]) \n",
            "dis1_forward_output_fake.shape : torch.Size([16, 1, 27, 27]) dis1_backward_output_fake.shape : torch.Size([16, 1, 28, 28]) \n",
            "endloop : 25.349\n",
            "step : 20  d_loss : 0.7266499400138855  g_loss : 2.779632329940796  lr : 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "            Crop(1000),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "        ])\n",
        "test_s_dataset = DatasetFolder(args.test_s_path, 0, transform=test_transform)\n",
        "test_s_loader = DataLoader(test_s_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "test_t_dataset = DatasetFolder(args.test_t_path, 1, transform=test_transform)\n",
        "test_t_loader = DataLoader(test_t_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "dataiter = iter(test_s_loader)\n",
        "x_test, _  = next(dataiter)\n",
        "dataiter1 = iter(test_t_loader)\n",
        "y_test_big, _  = next(dataiter1)\n",
        "\n",
        "y_test = transforms.Resize(x_test.size()[-2:], interpolation=Image.BICUBIC)(tensor2pil(y_test_big[0])) # To resize, tensor should be converted to pillow image.\n",
        "y_test = transforms.ToTensor()(y_test)\n",
        "y_test = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(y_test)\n",
        "y_test = torch.unsqueeze(y_test,0)\n",
        "print(y_test[0].size())\n",
        "print(x_test[0].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAOKd0GfZygD",
        "outputId": "4306ddce-14eb-4336-f131-c9928b7135b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 480, 492])\n",
            "torch.Size([3, 480, 492])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3,480,490)\n",
        "#x_lum = rgb_to_ycbcr(x)[:,0]\n",
        "(x[:,0]).size()"
      ],
      "metadata": {
        "id": "0q1AVtoEn2xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7e74cd-3d5d-430f-d30f-f0f9603f8595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 490])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_0aQ2sFyWE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}